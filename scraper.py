# -*- coding: utf-8 -*-
"""scraper

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uB2uo-OJnQ7O1L1LIefD9m71WxP0emk3
"""

import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import pandas as pd
import re

def scrape_site(url):
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    driver = webdriver.Chrome(options=chrome_options)

    driver.get(url)
    time.sleep(5)

    soup = BeautifulSoup(driver.page_source, 'html.parser')

    # Utilizando Regex e XPath para extrair informações
    title = soup.find('h1') or soup.find('h2')
    description = soup.find('p')
    price = soup.find(lambda tag: tag.name == "span" and re.search(r'R\$\s*\d+', tag.text))

    driver.quit()

    return {
        "title": title.text.strip() if title else 'Não disponível',
        "description": description.text.strip() if description else 'Não disponível',
        "price": price.text.strip() if price else 'Não disponível'
    }

def scrape_multiple_sites(urls):
    all_data = []
    for url in urls:
        data = scrape_site(url)
        data['url'] = url
        all_data.append(data)
    return all_data

def save_to_csv(data):
    df = pd.DataFrame(data)
    return df.to_csv().encode('utf-8')

def save_to_excel(data):
    df = pd.DataFrame(data)
    return df.to_excel(index=False, engine='openpyxl')